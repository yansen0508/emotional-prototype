<!DOCTYPE HTML>
<html>
	<head>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-109046767-2');
		</script>


		<!-- CHANGE HERE -->
		<title>Exploring Mental Prototypes by an Efficient Interdisciplinary Approach: Interactive Microbial Genetic Algorithm</title>
		<!-- ----------- -->
		<script src="https://cdn.jsdelivr.net/npm/before-after-slider@1.0.0/dist/slider.bundle.js"></script>


		<script
		  defer
		  src="https://unpkg.com/img-comparison-slider@7/dist/index.js"
		></script>
		<link
		  rel="stylesheet"
		  href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"
		/>

		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600|Source+Code+Pro" rel="stylesheet" />
		<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">

		<!-- The loading of KaTeX is deferred to speed up page rendering -->
		<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>

		<!-- To automatically render math in text elements, include the auto-render extension: -->
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
		<script src="js/skel.min.js">
		{
			prefix: 'css/style',
			preloadStyleSheets: true,
			resetCSS: true,
			boxModel: 'border',
			grid: { gutters: 30 },
			breakpoints: {
				wide: { range: '1200-', containers: 1140, grid: { gutters: 50 } },
				narrow: { range: '481-1199', containers: 960 },
				mobile: { range: '-480', containers: 'fluid', lockViewport: true, grid: { collapse: true } }
			}
		}
		</script>

		<style>
			table tr th, table tr td {
			text-align: center;
			vertical-align: middle;
			border: 0px solid white;
			border-collapse: collapse;
			}
		</style>

		<style type="text/css">a {text-decoration: none}</style>

	</head>

	<body>

		<div id="site_content">
		<div class="container">

		<!-- Features -->
		<div class="row">
			<section class="12u">

				<h2 style="text-align: center;">
					Exploring Mental Prototypes by an Efficient Interdisciplinary Approach: Interactive Microbial Genetic Algorithm
				</h2>

				<p style="text-align: center;">
					Sen Yan&emsp;&emsp; Catherine Soladié&emsp;&emsp; Renaud Séguier&emsp;&emsp;
				</p>
                
				<p style="text-align: center; line-height: 100%;">
					CentraleSupélec, IETR UMR CNRS 6164, France <br>
					email:{first_name}.{last_name}@centralesupelec.fr <br>
				</p>
				
				
				<p style="text-align: center;">
					 <a href="https://" target="_blank" rel="noopener">Article</a> |
					<!-- <a href="#audio">Audio examples</a> |  -->
					 <a href="https://youtu.be/GwvC2u9r01o">Video demo</a> |    
					<a href="#supplementary material">Supplementary material</a>|
					<a href="#acknowledgement">Acknowledgement</a>
				</p>

				<table>
					<td text-align: center;>
						<img src="demos/maintext/IMGA.png" width="1100" class="center" title="Framework of our IMGA"/> 
						<p <span style="font-size: strong; color:gray;"><a name="IMGA"></a><strong>Framework of our IMGA</strong></span> </p>
					</td>
					
				</table>

				<!-- Abstract -->
				<hr>
				<strong><span style="font-size: large;"><a name="abstract"></a>Abstract</span></strong>
				<hr>
				
				<p style = "text-align: justify; vertical-align: top;"> Facial expression-based technologies have flooded our daily lives. However, most technologies are limited to Ekman's basic facial expressions and rarely deal with more than ten emotional states.
                    This is not only due to the lack of prototypes for complex emotions but also the time-consuming and laborious task of building an extensive labeled database.
                    To remove these obstacles, we were inspired by a psychophysical approach for affective computing, so-called the reverse correlation process (RevCor), to extract mental prototypes of what a given emotion should look like for an observer. 
                    We proposed a novel, efficient, and interdisciplinary approach called Interactive Microbial Genetic Algorithm (IMGA) by integrating the concepts of RevCor into an interactive genetic algorithm (IGA).
                    Our approach achieves four challenges: online feedback loop, expertise-free, velocity, and diverse results.
                    Experimental results show that for each observer, with limited trials, our approach can provide diverse mental prototypes for both basic emotions and emotions that are not available in existing deep-learning databases. </p>
				
				<!-- method & results -->
				<hr>
				<span style="font-size: large;"><a name="results"></a><strong>Method & Experiment</strong></span>
				<hr>
				<div style="width:100%; text-align: center;">
				<video controls autoplay  muted preload="auto" width="1000" height="700">
				  <source src="demos/Video/108-fg.mp4" type="video/mp4">
				  <source src="demos/Video/108-fg.ogg" type="video/ogg">
				  Your browser does not support the video tag.
				</video>
				</div>
				If your browser does not support the video tag,
				<span style="font-size: strong;"><a href="https://youtu.be/GwvC2u9r01o"><strong>click here.</strong></a></span>

				<!-- Supp mat -->
				<hr>
				<span style="font-size: large;"><a name="supplementary material"></a><strong>Supplementary materials</strong></span>
				<hr>
				<p style = "text-align: justify; vertical-align: top;">According to the structural order of our manuscript, we present the supplementary figures and demonstrations for better understanding:</p>
				<ul>
				  <li> <span style="font-size: strong;"><a href="#supp1"><strong>1) An example of the population initialization. </strong></a></span></li>
				  <li> <span style="font-size: strong;"><a href="#supp2"><strong>2) The details for setting GA parameters. </strong></a></span> </li>
				  <li> <span style="font-size: strong;"><a href="#supp3"><strong>3) All representative prototypes from the 12 observers and the state-of-the-art prototypes. </strong></a></span> </li>
				  <li> <span style="font-size: strong;"><a href="#supp4"><strong>4) Details of the experiment duration. </strong></a></span> </li>
				  <li> <span style="font-size: strong;"><a href="#supp5"><strong>5) Subjective evaluation: Baseline computation. </strong></a></span> </li>
				  <li> <span style="font-size: strong;"><a href="#supp6"><strong>6) Subjective evaluation: Final ranking of representative prototypes. </strong></a></span> </li>
				</ul>
				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp1"></a><strong>Population initialization</strong></span>
				</p>
				<table>
					<td text-align: center;>
						<img src="demos/maintext/pop_init.png" align="center" width="600" title="population initialization"/> 
						<p <span style="font-size: strong; color:gray;"><a name="IMGA"></a><strong>Example of population initialization. We use GANimation to generate different facial expressions (from #1 to #20) from a neutral face (#0 in red) and different AU (action unit) vectors.</strong></span> </p>
					</td>
				</table>
				Differing from the majority of facial manipulation techniques that are designed to modify high-level attributes such as hair color, gender, age, or emotional expressions,
				<a href="http://www.albertpumarola.com/research/GANimation/index.html">GANimation</a> is trained by the low-level attributes, which are action units (AUs).
				As the AU vectors were randomly initialized, some facial expressions do not correspond to any emotional state, such as #2 with the activation of AU4 (brow lowerer), AU5 (upper lid raiser), and AU12 (lip corner puller). 

				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp2"></a><strong>GA parameter settings</strong></span>
				</p>
				It is necessary to find a set of relatively appropriate GA parameters before observers perform the perceptual experiments.
These parameters are the crossover rate \(cr\), the mutation rate \(mr\), the population size \(N\), the thresholds of the constraint automaton \([T1, T2, T3]\) and constants for the population evaluation \(\alpha,\beta\).
Some parameters were set according to the suggestion from the litterature of MGA: \(cr=0.5,mr=0.03\).
Some parameters were taken empirically.
Considering user fatigue, the system needs to finally converge in a limited time (we set it to 15 minutes).
We need to calibrate the time-sensitive parameters: population size \(N\) and thresholds of the constraint automaton \([T1, T2, T3]\), since the first one is related to the number of trials for each iteration and the second one is related to the degree of the system convergence.
Empirically, we set \(\alpha=\beta=0.5\) (inter-population similarity and intra-population similarity are both important.) and \(T1=T2=T3-0.05\) (a slightly higher threshold for the stop condition).

We simulated the perceptual experiments by replacing the real observers with an automatic facial expression recognition system.
We took the VGG-19 pretrained model as the simulator which outperformed the facial expression recognition tasks in FER2013 dataset (acc=73.112\%) and in CK+ dataset (acc=94.64\%).
The simulators performed the experiments with different GA parameter settings.

For each experimental task of basic emotions (happiness, sadness, anger), the simulator gave each individual in the last generation a score \(r_i\), which is the output value of the corresponding emotional class in the softmax layer of VGG-19.
We used the average of all individuals to represent the entire population, \(R_k=\frac{1}{N}\sum_{i=1}^{N}r_i\), where \(N\) is the population size and \(k\) is the simulation number. 
For each experimental task, the simulator was performed 30 times.
Thus, there were, in total, 90 simulations (30 simulations/task \(\times\) 3 experimental tasks) for a given set of \(N, T1\).
We evaluated the simulations by using the average score to represent all populations for the three experimental tasks, \(R_{f}=\frac{1}{90}\sum_{i=1}^{90}R_k\).
				<table>
					<td text-align: center;>
						<img src="demos/maintext/sim_pop_eva.png" width="500" />
						<p <a name="sim score"></a>a) The score \(R_f\) assessed by VGG-19</p>
					</td>
					<td>
						<img src="demos/maintext/sim_nb_trials.png" width="550" />
						<p <a name="sim trials"></a>b) Number of trials (left Y-axis) and corresponding times (right Y-axis)</p>
					</td>
				</table>

With different combinations of the population size \(N\) and the first threshold \(T1\), we present the final scores (in Fig.<a href="#sim score">a</a>) and the number of trials required for our IMGA (in Fig.<a href="#sim trials">b</a>). 
In Fig.<a href="#sim score">a</a>, there are two scores that are rather high: \(R_{f}=0.87\) (with \(N=10\) and \(T1=0.9\)) and \(R_{f}=0.88\) (with \(N=20\) and \(T1=0.9\)).
Through the attempts of the authors to perform several perception experiments, the response time for each trial was, on average, less than 3 seconds.
We set, on average, 3 seconds per trial as the estimated time. 
Note that the time limit is 15 minutes (dotted line in Fig.<a href="#sim trials">b</a>).
We estimated that it took about 8.5 minutes (with \(N=10\), \(T1=0.9\)) and 13.8 minutes (with \(N=20, T1=0.9\)) for one observer to perform one perceptual experiment. 
Like most genetic algorithms, there is always a trade-off between time and solution diversity.
Considering the diversity of the mental prototypes, we finally set \(N=20\) and \(T1=0.9, T2=0.9, T3=0.95\). 
Indeed, in an initialized population of \(N=20\) individuals, each AU has already been activated an average of 3.75 times.

				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp3"></a><strong>All representative prototypes</strong></span>
				</p>
				We list all representative prototypes of observers as well as the state-of-the-art prototypes (denoted by "Ek" and "Yu", in pink) for comparison.
Since GANimation does not provide the option to edit AU16 (lower lip depressor), we replace AU16 with AU25 (lips part) to reconstruct the anger prototype of Yu et al.
				<img src="demos/maintext/proto.png" width="800" class="center"/>

				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp4"></a><strong>Experiment duration</strong></span>
				</p>
				In Fig.<a href="#trials">a</a>, box plots (described by the maximum, the minimum, the median, the average, and the first and third quartiles) show the number of trials performed by observers.
The corresponding time for perceptual experiments is shown in Fig.<a href="#time">a</a>.
Since the response time for each observer in each trial is different, there are subtle differences between Fig.<a href="#trials">a</a> and Fig.<a href="#time">a</a>.
On average, it took about 10.8 minutes (330 trials) for one observer to perform the perceptual experiment.
				<table>
					<td text-align: center;>
						<img src="demos/maintext/trials.png" width="500" />
						<p <a name="trials"></a>Number of trials</p>
					</td>
					<td>
						<img src="demos/maintext/time.png" width="500" />
						<p <a name="time"></a>Duration (in minutes)</p>
					</td>
				</table>

				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp5"></a><strong>Baseline computation</strong></span>
				</p>
				Here, we present the computation for the baseline of the first measurement: 
the proportion of observers who still chose at least one representative prototype of theirs.
The baseline can be regarded as the probability for one observer who still chose at least one of his/her representative prototypes.
For each evaluation task, we define the set of all representative prototypes as \(P\) and the set of the representative prototypes of one observer as \(p_{i}\). 
We compute the probability for one observer who did not select his/her representative prototypes:
\(\frac{C_5^{|P|-|p_{i}|}}{C_5^{|P|}}\), where \(C\) is the mathematical combination function and the operator \(|.|\) is the cardinality of a set.
Hence, we can obtain the probability for one observer who still chose at least one of his/her representative prototypes:
\(1-\frac{C_5^{|P|-|p_{i}|}}{C_5^{|P|}}\).
The baseline of one evaluation task is the average probability for the 12 observers:
\(\frac{1}{12}\sum_{i=1}^{12}\left(1-\frac{C_5^{|P|-|p_{i}|}}{C_5^{|P|}}\right)\).

				<p style="text-align: left;">
				<span style="font-size: large;"><a name="supp6"></a><strong>Final ranking of representative prototypes</strong></span>
				</p>
				For the second measurement: we sorted all the prototypes according to the proportion selected by observers and displayed the top 5 prototypes with the highest proportions.
We displayed by images the top-5 prototypes that observers chose the most. The prototype number is marked on the top left of the image. There is no state-of-the-art prototype appearing in the top-5 prototypes.
Since our representative prototype #11-sadness is identical to Ekman-sadness, we merged them and marked them by "Ek/11".
The state-of-the-art prototypes are less preferred by observers.
				<table>
					<td text-align: center;>
						<img src="demos/maintext/happyeva_fig.png" width="500" />
						<p <a name="trials"></a>Happiness</p>
					</td>
					<td>
						<img src="demos/maintext/sadeva_fig.png" width="500" />
						<p <a name="time"></a>Sadness</p>
					</td>
				</table>
				<table>
					<td text-align: center;>
						<img src="demos/maintext/angryeva_fig.png" width="500" />
						<p <a name="trials"></a>Anger</p>
					</td>
					<td>
						<img src="demos/maintext/confidenteva_fig.png" width="500" />
						<p <a name="time"></a>Confidence</p>
					</td>
				</table>

			<!-- acknowledgement -->
			<hr>
			<span style="font-size: large;"><a name="acknowledgement"></a><strong>Acknowledgement</strong></span>
			<hr>
			<p style="text-align: left;">
			This work was supported by Randstad France and ANR REFLETS. </p>
			
			<img src="demos/logo/CentraleSupelec.png" width="200" class="left" />
	
			<img src="demos/logo/Randstad.png" width="200" class="right" />
			
			
			
			</section>
		</div>
	</div>
</div>

</body>
</html>

